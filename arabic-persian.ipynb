{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2503457,"sourceType":"datasetVersion","datasetId":1515974}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np \nimport pandas as pd \nimport os\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom sklearn import preprocessing \nfrom torchvision import transforms\nimport imageio as iio\nfrom PIL import Image\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-01T23:33:02.152856Z","iopub.execute_input":"2023-12-01T23:33:02.153344Z","iopub.status.idle":"2023-12-01T23:33:02.161858Z","shell.execute_reply.started":"2023-12-01T23:33:02.153312Z","shell.execute_reply":"2023-12-01T23:33:02.160241Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"names = ['Amin','Farzad','Maziar','Mehrdad','Sina','Soheil','Vahid']\nname_file = {name:[] for name in names}\n\nfor dirname, _, filenames in os.walk('/kaggle/input/arabicpersian-handwritten-cities-for-postal-apps/scan splited/scan splited'):\n    filenames = sorted(filenames) # i want it to be on the same order\n    for filename in filenames:\n        name = dirname[89:]\n        name_file[name].append(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:33:02.164305Z","iopub.execute_input":"2023-12-01T23:33:02.164759Z","iopub.status.idle":"2023-12-01T23:33:03.624252Z","shell.execute_reply.started":"2023-12-01T23:33:02.164702Z","shell.execute_reply":"2023-12-01T23:33:03.623195Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# I just want to take subset of the dataset","metadata":{}},{"cell_type":"code","source":"alphabit = \"ابتثجحخدذرزسشصضطظعغفقكلمنهويئ\"\nnum_output = len(alphabit)\nalphabit = [a for a in alphabit]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:33:03.626007Z","iopub.execute_input":"2023-12-01T23:33:03.627565Z","iopub.status.idle":"2023-12-01T23:33:03.634657Z","shell.execute_reply.started":"2023-12-01T23:33:03.627511Z","shell.execute_reply":"2023-12-01T23:33:03.632844Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def pad_words(targets):\n    \"\"\"\n    Padding words to make all of them on the same size\n    \"\"\"\n    targets_new = []\n    for target in targets:\n        pad = 6-len(target)\n        targets_new.append(np.concatenate((np.zeros(pad),target),axis=0).astype(np.longlong))\n        \n        \n    return targets_new","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:33:03.636951Z","iopub.execute_input":"2023-12-01T23:33:03.637792Z","iopub.status.idle":"2023-12-01T23:33:03.650502Z","shell.execute_reply.started":"2023-12-01T23:33:03.637748Z","shell.execute_reply":"2023-12-01T23:33:03.649369Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"words = (\" خيابان ميدان نمين ديلم مهران كاشان برحوار نائين البرز يزد تبريز سيريك\") # only these words will be considerd\nwords = words*4 # four persons wrote these words\nwords = words.split(' ')\nwords.__delitem__(0) # first element is just empty\n\n\nimgs = []\nfor name,files in name_file.items():\n    if  name.__eq__('Maziar') or name.__eq__('Mehrdad') or name.__eq__('Sina'):\n        continue\n    for i,file in enumerate(files):\n        if i == 12:\n            break\n        imgs.append(file)\n    \n\ntargets = [[w for w in word] for word in words]\nlbl_enc = preprocessing.LabelEncoder()\nlbl_enc.fit(alphabit)\ntargets_enc = [lbl_enc.transform(word)+1 for word in targets]\ntargets_enc = pad_words(targets_enc)\ndataset = {'imgs':imgs, \"labels\":targets_enc}\ndf = pd.DataFrame(dataset)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:33:03.653382Z","iopub.execute_input":"2023-12-01T23:33:03.654100Z","iopub.status.idle":"2023-12-01T23:33:03.684418Z","shell.execute_reply.started":"2023-12-01T23:33:03.654055Z","shell.execute_reply":"2023-12-01T23:33:03.683315Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"class Mydataset(Dataset):\n    \n    def __init__(self,df,trans=None):\n        self.df = df\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        x = Image.open(self.df['imgs'][idx]).convert('RGB')\n        if trans:\n            x = trans(x)\n        y = self.df['labels'][idx] +1 \n        return (x,y)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:33:03.686215Z","iopub.execute_input":"2023-12-01T23:33:03.687061Z","iopub.status.idle":"2023-12-01T23:33:03.696527Z","shell.execute_reply.started":"2023-12-01T23:33:03.687016Z","shell.execute_reply":"2023-12-01T23:33:03.695051Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# std = torch.tensor([0.0603, 0.0606, 0.0347])\n# mean = torch.tensor([[0.8753, 0.8924, 0.9244]])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:33:03.697987Z","iopub.execute_input":"2023-12-01T23:33:03.698430Z","iopub.status.idle":"2023-12-01T23:33:03.711667Z","shell.execute_reply.started":"2023-12-01T23:33:03.698384Z","shell.execute_reply":"2023-12-01T23:33:03.709966Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"trans = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(size=(20,80)),\n    transforms.Normalize(mean=(0.8753,0.8924,0.9244),std=(0.0603,0.0606,0.0347))\n                           ])\nall_imgs = []\ndataset = Mydataset(df,trans)\n\n    \n\ndataloader = DataLoader(dataset,batch_size=48)\nfor batch in dataloader:\n    print(batch[1].shape)\n    break\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:33:03.713564Z","iopub.execute_input":"2023-12-01T23:33:03.714247Z","iopub.status.idle":"2023-12-01T23:33:04.177276Z","shell.execute_reply.started":"2023-12-01T23:33:03.714202Z","shell.execute_reply":"2023-12-01T23:33:04.175636Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([48, 6])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class MyCNN(nn.Module):\n    \n    def __init__(self):\n        super(MyCNN,self).__init__()\n        self.conv1 = nn.Conv2d(3,12,3,2)\n        self.conv2 = nn.Conv2d(12,36,3,2)\n        self.conv3 = nn.Conv2d(36,84,3,2)\n        \n    def forward(self,x):\n        x = self.conv1(F.relu(x))\n        x = self.conv2(F.relu(x))\n        x = self.conv3(F.relu(x))\n        return x\n    \nclass MyRNN(nn.Module):\n    \n    def __init__(self,input_size,hidden_size,num_layers,bi):\n        super(MyRNN,self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers        \n        self.gru = nn.GRU(input_size,hidden_size,num_layers,bidirectional=bi,batch_first=True)\n        self.linear1 = nn.Linear(hidden_size,hidden_size)\n        self.linear2 = nn.Linear(hidden_size,num_output+1) # +1 for the blank ctc\n        \n        \n    def forward(self,x):\n        h0 = torch.zeros(2*self.num_layers,x.size(0),self.hidden_size)\n        out,hn = self.gru(x,h0)\n        print(out.shape)\n        out = self.linear1(F.relu(out))\n        print(out.shape)\n        out = self.linear2(F.log_softmax(out,dim=1))\n        return out\n        \n        \n# class denseLayer(nn.Module):\n#     def __init__\n    \n    \nclass MyModel(nn.Module):\n    \n    def __init__(self,cnn,rnn:\n        self.cnn = cnn\n        self.rnn = rnn\n        \n    def forward(self,x):\n        x = self.cnn(x)\n        print(x.shape) # --> (bs,c,w,h)\n        # for rnn , we need (time_stamps,bs,featuers)\n        x = x.view(-1,x.shape(1),x.shape(2)*x.shape(3)) # --> (bs,c,w*h)\n        # i will assume that num of channels is the time_stamps\n        x = x.permute(1,0,2)\n        x = self.rnn(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:33:04.179894Z","iopub.execute_input":"2023-12-01T23:33:04.180876Z","iopub.status.idle":"2023-12-01T23:33:04.198206Z","shell.execute_reply.started":"2023-12-01T23:33:04.180830Z","shell.execute_reply":"2023-12-01T23:33:04.196897Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:33:04.200314Z","iopub.execute_input":"2023-12-01T23:33:04.200874Z","iopub.status.idle":"2023-12-01T23:33:04.224041Z","shell.execute_reply.started":"2023-12-01T23:33:04.200832Z","shell.execute_reply":"2023-12-01T23:33:04.222840Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"torch.Size([5, 3, 2])"},"metadata":{}}]},{"cell_type":"code","source":"# !git remote add origin https://github.com/Mohamad-Atif1/CTC_loss_PyTorch.git\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T23:33:04.226827Z","iopub.execute_input":"2023-12-01T23:33:04.227749Z","iopub.status.idle":"2023-12-01T23:33:05.357613Z","shell.execute_reply.started":"2023-12-01T23:33:04.227687Z","shell.execute_reply":"2023-12-01T23:33:05.355841Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"fatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n","output_type":"stream"}]}]}